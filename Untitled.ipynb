{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Výběr snímků z videa\n",
    "## Focus measure\n",
    "[https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/]\n",
    "\n",
    "Nejkvalitnější snímky z videa jsou ty s nejmenším rozmazáním. Méně rozmazaný snímek zároven zřejmě bude mít výraznější hrany než ten více rozmazaný. Hrany detekujeme pomocí konvoluce Laplaceova operátoru s vybraným snímkem. Výsledný (absolutní) průměr této matice nám bude sloužit jako nástroj k porovnávání rozmazání mezi snímky.\n",
    "Tato metoda není úplně přesná, protože se scéna na snímcích konstantě mění, nicméně se nemění tak dramaticky, aby nebyla použitelná.\n",
    "\n",
    "## Velikost okna\n",
    "Předpokládáme rychlost kamery 20 cm/s a frame rate 30FPS ,tedy posun 2/3 cm/frame.\n",
    "Chceme aby se dva snímky překrývali alespon ze 1/3 (při menším překryvu by počítání tr.mat mohlo být obtížné/nepřesné) a zároven se nepřerývali více než ze 1/2 (minimalizace počtu snímků).\n",
    "Předpokládáme, že snímek zobrazuje vertikálně 1 m ve skutečnosti. Pak se dostáváme na hodnotu okna 37-75 framů.\n",
    "\n",
    "Nápad : Počítání rychlosti kamery. 2 kola\n",
    "1) Vyberu dostatečně ostré snímky (jen na začátku/ celé video?). Možná budu potřebovat pevné okno? \n",
    "\n",
    "2) Spočítám si projektivní tr. matice.\n",
    "\n",
    "3) Pomocí velikosti posunu tr.matice a časovou vzdáleností spočítám průměrnou rychlost.\n",
    "\n",
    "4) Přes známou rychlost spočítám přesnější okno\n",
    "\n",
    "\n",
    "## Error threshold\n",
    "Hranice chyby je spočítána jako odchylka vybraného snímku. Tato hodnota je porovnávána s chybou transformace. Ta je spočítána jako sum(abs(I1-I2)) kde I1,I2 jsou překrývající se části snímků.\n",
    "Pokud se chyba žádného snímku nevejde pod ErrThresh, je vybrán snímek s nejmenší chybou transformace v daném okně."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% můj kod v matlabu jak počítám ErrThresh\n",
    "I = rgb2gray(read(vid, frameList(i)));\n",
    "st = round(sum(I(:))/(1080*1920));\n",
    "A = uint8(ones(1080,1920)*st);\n",
    "B = (I-A).^2;\n",
    "b = sum(B(:))/(1080*1920);\n",
    "errThresh = sqrt(b);\n",
    "\n",
    "% co by mělo správně být\n",
    "mean     = sum(x)/length(x)\n",
    "variance = sum((x - mean(x)).^2)/(length(x) - 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "Pro všechny snímky ve videu je spočítána focus measure. Všechny ostatní práce se snímky je postupně dělána ve vyhrazených oknech, což je soubor snímků vzdálený od předchozího vybraného snímku minimální a maximální vzáleností(vzdáleností je myšleno časové pořadí snímků). Pro první průchod je jednoduše vybrán snímek s největší fm a spočítán ErrThresh pomocí něj. Zbytek cyklu probíhá takto:\n",
    "\n",
    "1) Všechny snímky okna jsou v sestupném pořadí seřazeny podle jejich focus measure.\n",
    "\n",
    "2) Snímky jsou jeden po druhém registrovány na vybraný snímek z předchozího okna, dokun není nalezen snímek s chybou transformace menší než ErrThresh. Pokud ani jeden není menší, je vybrán snímek s nejmenší chybout tr.\n",
    "\n",
    "3) Tento snímek je přidán do konečného výběru snímků (možná počítat ErrThresh pro každý vybraný snímek, a ne jen podle prvního??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registrace panoramatu\n",
    "[https://www.mathworks.com/help/vision/ug/feature-based-panoramic-image-stitching.html]\n",
    "\n",
    "## computeTforms\n",
    "Transfromační matice jsou počítány následující způsobem:\n",
    "Snímek je převeden do černobílý (1 vrstva místo 3 u RGB) a jsou naněm detekovány body pomocí SURF detektoru. Z těchto bodů jsou vybrány příznaky, které jsou následně spárovány s příznaky z předchozího snímku.\n",
    "Tyto 2 seznamy jsou poté použity funkcí estimateGeometricTransform, která spočítá odpovídající tr. matici.\n",
    "Pro potřeby panoramatu je použita affiní transformace, protože u projektivní dochází při součinu matic ke velké chybě.\n",
    "\n",
    "Pokud chceme, aby referenčním snímkem v panoramatu byl jiný než první, je potřeba je přepočítat pomocí \n",
    "tforms(j).T = tforms(j).T * Tinv.T; kde Tinv je inverze ref. snímku.\n",
    "\n",
    "## computeLimits\n",
    "Je třeba znát rozměry výsledného panoramatu, čehož docílíme transformací 4 bodů okrajů u všech snímku a následným výběrem těch nejzaších. V tomto kroce je dobré ověřit velikost pro výsledné panorama, pokud je příliš velká, evidentně někde nastala chyba.\n",
    "\n",
    "## createPanorama\n",
    "Díky znalosti rozměrů panoramatu si můžeme vytvořit konečný souřadný systém, do kterého nejdříve vložíme masku (opět 4 kraje původního snímku) a následně do místa masky vložíme transformovaný snímek. (Masku pak můžeme využít na zobrazení vizualizaci poskládání všech snímků)\n",
    "\n",
    "## Omezení \n",
    "Jak bylo výše uvedeno, u dlouhých panoramat s jakými pracujeme my, je třeba použít affiní, či podobnostní transformaci, nebot při součinu projektivních transformací dochází ke stále větší a větší chybě, a při více než 5-7 snímcích ani nelze žádné panorama vytvořit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeatability rate\n",
    "\n",
    "Repeatability rate definujeme jako počet stejných bodů vyskytujících se na obou snímcích vzhledem k celkovém počtu detekovaných bodů. Z detekovaných bodů vybíráme pouze ty, které jsou součástí scény vyskytujících se na obou snímcích.\n",
    "\n",
    "![title](aaa.png)\n",
    "\n",
    "Je důležité vzít v úvahu nepřesnost detekce. Korespondující bod se nebude nacházet přesně daném místě určeném maticí, ale v jeho $\\epsilon$-okolí. $\\epsilon$-reapeatabilitu definujeme jako R<sub>i</sub>($\\epsilon$) = {(x<sub>i</sub>,x<sub>j</sub>)|dist(H<sub>ij</sub>x<sub>i</sub>, x<sub>j</sub>) < $\\epsilon$}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vliv rozmazání na registraci\n",
    "\n",
    "# Vliv artefaktů"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sčítání cenovek\n",
    "\n",
    "V této části si ukážeme využití výše zmíněných metod na problému počítání cenovek. V podstatě jde o schopnost detekce stejného objektu na více snímcích a následném rozdělění tak, aby každý snímek měl \"své\" cenovky. Pro toto je využití vlastností transfromačních matic ideální.\n",
    "\n",
    "Pro samotnou detekci cenovek byla využita typ síte YOLOv5[https://github.com/ultralytics/yolov5] natrénována na cenovky od firmy DataSentics.\n",
    "Ta vyniká především pro svou rychlost i malou velikost, čímž je vhodná pro využití v mobilních zařízeních.\n",
    "Vstupem je snímek a výstupem list souřadnic bounding boxů.\n",
    "\n",
    "Snímky jsou získany z videa postupem výše uvedeném. Mezi těmito snímky je spočítany matice projektivní transformace a je také (odděleně) vygenerováno panorama pomocí affiní transformace. Toto panoram slouží k ručnímu\n",
    "sčítání cenovek.\n",
    "\n",
    "V každém cyklu jsou vzaty 3 po sobě jsoucí snímky, v nichž jsou následně detekovány cenovky. První a třetí bboxy jsou převedeny transformací do souřadného systému druhého snímku. Poté jsou přes Python knihovnu Shapely[] souřadnice bboxů převedeny do polygonů pro snadnější manipulaci. \n",
    "\n",
    "Polygony prvního a druhého snímku jsou dávány přes sebe a pokud je nalezen překryv, polygon je odstraněn z list ve 3. a zůstává pouze ve 1.\n",
    "Následně jsou podobným způsoben kontrolovány polygony 1.&2. a 3.&2. Při nalezení překryvu ale o tom, ze kterého listu bude polygon smazán rozhoduje velikost obsahu cenovky.\n",
    "\n",
    "Po prvním cyklu neprochází již detekované snímky sítí znovu, místo toho jsou brány seznamy z předchozích 2 cyklů.\n",
    "Díky tomu nedochází k duplikaci cenovek na různých snímcích. Mohlo by v nějakých případech dojít k duplikaci, pokud by se objekt objevil na 4 a více snímcích po sobě, nicméně k tomu v podstatě nedochází, vzhledem k tomu že snímky jsou vybrány z videa, tak, aby neměly tak velký překryv. Nehledě na to, že při součinu projektivních transformačních matic se rychle kumuluje chyba po několika cyklech by se bboxy přestaly překrývat.\n",
    "\n",
    "\n",
    "## Omezení\n",
    "Sít má problém s cenovkami velmi blízko u sebe a najde neexistující cenovky. Také má problém správně detekovat cenovky vyloženě přes sebe a neobvyklých formátů(elektronické, moc dlouhé)\n",
    "\n",
    "## Výsledky\n",
    "ručně/program\n",
    "\n",
    "set30-188/190\n",
    "\n",
    "set34-102/96\n",
    "\n",
    "set31\n",
    "set32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
