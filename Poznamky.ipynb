{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Úvod\n",
    "\n",
    "## DataSentics\n",
    "\n",
    "Tuto bakalářskou práci jsem vypracovala ve spolupráci se společností DataSentics, jako možné rozšíření jednoho z jejich produktů ShelfInspector. Ten zpracovává snímky regálů se zbožím pomocí natrénovaných neuronových sítí. Následně je pak díky tomu schopen poskytnout informace jako je počet a pozice konkrétního produktu, ale také jeho současnou cenu v daném regálu. V současné době jsou používány jednotlivě nafocené snímky, a právě proto je cílem mé práce prozkoumat možnosti (a omezení) analýzy regálů z videa pomocí metod registrace obrazu.\n",
    "\n",
    "## Registrace\n",
    "\n",
    "Registrace obrazu je proces kombinování dvou a více snímků pořízených z odlišných pohledů, času\n",
    "nebo zdrojů do jednoho souřadného sytému [2]. Je například často používaná při zpracovávání dat ze\n",
    "satelitů,v počítačovém vidění nebo medicíně.\n",
    "Registraci lze rozdělit do pěti kroků:\n",
    "1. detekce příznaků\n",
    "2. korespondence příznaků\n",
    "3. odhad transformace\n",
    "4. transformace snímku a resampling\n",
    "\n",
    " Resampling bude vynechán, nebot pro potřeby této práce si vystačíme s jednoduchým přeložením jednoho snímku přes druhý.\n",
    " \n",
    " \n",
    " \n",
    "## Příznaky a jejich detekce\n",
    "\n",
    "Příznaky lze označit jako významné body obrázku, jako jednoduchý příklad lze uvést hrany nebo okraj objektu zobrazovaného na snímku. Je žádoucí, aby byl invariantní, nebo alespon dostatečně robustní vůči ... jako rotace, škálování, osvětlení, rozmazání, šumu či změny v úhlu pohledu. Tuto schopost později nazveme opakovatelností(repeatability), tedy schopnost (daného detektoru) najít stejný bod na snímcích pořízenými za různých podmínek.\n",
    "\n",
    "Kromě samotné polohy bodu jsou ukládaány tzv. deskriptory, které obsahují informace o intenzitě? bodu a jeho okolí.\n",
    "\n",
    "V této práci byl jako detektor použit SURF[]...(popis fungování) \n",
    "\n",
    "## korespondence\n",
    "\n",
    "Následně je spočítána vzdálenost mezi deskriptory a sobě nejbližší body prohlášeny za páry.\n",
    "\n",
    "Bruteforce vs kNN(exhaustive vs approximate)\n",
    "\n",
    "# odhad transformace\n",
    "\n",
    "## RANSAC\n",
    "\n",
    "Je náhodně vzato n namatchovaných párů. Čím je jich méně, tím lépe, ale jsou třeba alespon 4 páry pro projektivní a 3 pro affiní transformaci. Pomocí nich je odhadnut model transformace. Následně jsou všechny ostatní body ztransformovány podle této matice a ty které se nachází dostatečně blízko svému párovému bodu jsou prohlášeny za inliers, ostatní za outliers. Model transformace s největším počtem inliers je zvolen jako výsledný.\n",
    "\n",
    "## affiní, projektivní tr.\n",
    "\n",
    "(definice hom.souradnice)\n",
    "Necht je souradni bodu x=(x1,x2). Pak jeho homogenní souřadnici je x=(x1,x2,1).\n",
    "Bod y=(y1,y2,y3) prevede do R2 jako y=(y1/y3,y2/y3). (x1,x2,0) značí že je bod v nekonečnu\n",
    "\n",
    "Výsledná matice je lineární transformací, která je pro naše potřeby bud projektivní nebo affiní.\n",
    "\n",
    "affiní, projektivní\n",
    "\n",
    "\n",
    "co se zachovává při affiní, projektvní?\n",
    "\n",
    "popis pomoci nehomogennich souradnic\n",
    "\n",
    "\n",
    "## transformace(pipelina?)\n",
    "\n",
    "Abycho předešli vzniku děr a problémech s neceločíselnými souřadnicím při transformaci bodu zdrojového obrazu, používáme zpětnou metodu. Ta spočívá v procházení bodů warpovaného obrazu a určení zdrojového bodu pro daný bod pomocí zpětné transformace.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Výběr snímků z videa\n",
    "mit video moc rychle\n",
    "## Focus measure\n",
    "[https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/]\n",
    "\n",
    "Nejkvalitnější snímky z videa jsou ty s nejmenším rozmazáním. Méně rozmazaný snímek zároven zřejmě bude mít výraznější hrany než ten více rozmazaný. Hrany detekujeme pomocí konvoluce Laplaceova operátoru s vybraným snímkem. Výsledný (absolutní) průměr této matice nám bude sloužit jako nástroj k porovnávání rozmazání mezi snímky.\n",
    "Tato metoda není úplně přesná, protože se scéna na snímcích konstantě mění, nicméně se nemění tak dramaticky, aby nebyla použitelná.\n",
    "\n",
    "## Velikost okna\n",
    "Předpokládáme rychlost kamery 20 cm/s a frame rate 30FPS ,tedy posun 2/3 cm/frame.\n",
    "Chceme aby se dva snímky překrývali alespon ze 1/3 (při menším překryvu by počítání tr.mat mohlo být obtížné/nepřesné) a zároven se nepřerývali více než ze 1/2 (minimalizace počtu snímků).\n",
    "Předpokládáme, že snímek zobrazuje vertikálně 1 m ve skutečnosti. Pak se dostáváme na hodnotu okna 37-75 framů.\n",
    "\n",
    "Nápad : Počítání rychlosti kamery. 2 kola\n",
    "1) Vyberu dostatečně ostré snímky (jen na začátku/ celé video?). Možná budu potřebovat pevné okno? \n",
    "\n",
    "2) Spočítám si projektivní tr. matice.\n",
    "\n",
    "3) Pomocí velikosti posunu tr.matice a časovou vzdáleností spočítám průměrnou rychlost.\n",
    "\n",
    "4) Přes známou rychlost spočítám přesnější okno\n",
    "\n",
    "100 frameu uprostred-spocitam rychlost z toho\n",
    "dynamicke zvetsivano/zmensovani okna podle predchoziho-lepsi\n",
    "\n",
    "## Error threshold\n",
    "\n",
    "Pro odhad dobré transformace použijeme následující metodu: Označíme smer. odchylku intenzity snímku jako errThresh. Tato hodnota nám bude sloužit jako prah při porovnávání s chybou transformace. Ta je spočítána jako sum(abs(I1-I2)) kde I1,I2 jsou překrývající se části snímků.\n",
    "Pokud se chyba žádného snímku nevejde pod ErrThresh, je vybrán snímek s nejmenší chybou transformace v daném okně.\n",
    "\n",
    "mean     = sum(x)/length(x)\n",
    "\n",
    "variance = sum((x - mean(x)).^2)/(length(x) - 1)\n",
    "\n",
    "std = sqrt(variance)\n",
    "\n",
    "\n",
    "## Pipeline\n",
    "Pro všechny snímky ve videu je spočítána focus measure. Všechny ostatní práce se snímky je postupně dělána ve vyhrazených oknech, což je soubor snímků vzdálený od předchozího vybraného snímku minimální a maximální vzáleností(vzdáleností je myšleno časové pořadí snímků). Pro první průchod je jednoduše vybrán snímek s největší fm a spočítán ErrThresh pomocí něj. Zbytek cyklu probíhá takto:\n",
    "\n",
    "1) Všechny snímky okna jsou v sestupném pořadí seřazeny podle jejich focus measure.\n",
    "\n",
    "2) Snímky jsou jeden po druhém registrovány na vybraný snímek z předchozího okna, dokun není nalezen snímek s chybou transformace menší než ErrThresh. Pokud ani jeden není menší, je vybrán snímek s nejmenší chybout tr.\n",
    "\n",
    "3) Tento snímek je přidán do konečného výběru snímků "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registrace panoramatu\n",
    "[https://www.mathworks.com/help/vision/ug/feature-based-panoramic-image-stitching.html]\n",
    "\n",
    "## computeTforms\n",
    "Transfromační matice jsou počítány následující způsobem:\n",
    "Snímek je převeden do černobílý (1 vrstva místo 3 u RGB) a jsou naněm detekovány body pomocí SURF detektoru. Z těchto bodů jsou vybrány příznaky, které jsou následně spárovány s příznaky z předchozího snímku.\n",
    "Tyto 2 seznamy jsou poté použity funkcí estimateGeometricTransform, která spočítá odpovídající tr. matici.\n",
    "Pro potřeby panoramatu je použita affiní transformace, protože u projektivní dochází při součinu matic ke velké chybě.\n",
    "\n",
    "Pokud chceme, aby referenčním snímkem v panoramatu byl jiný než první, je potřeba je přepočítat pomocí \n",
    "tforms(j).T = tforms(j).T * Tinv.T; kde Tinv je inverze ref. snímku.\n",
    "\n",
    "## computeLimits\n",
    "Je třeba znát rozměry výsledného panoramatu, čehož docílíme transformací 4 bodů okrajů u všech snímku a následným výběrem těch nejzaších. V tomto kroce je dobré ověřit velikost pro výsledné panorama, pokud je příliš velká, evidentně někde nastala chyba.\n",
    "\n",
    "## createPanorama\n",
    "Díky znalosti rozměrů panoramatu si můžeme vytvořit konečný souřadný systém, do kterého nejdříve vložíme masku (opět 4 kraje původního snímku) a následně do místa masky vložíme transformovaný snímek. (Masku pak můžeme využít na zobrazení vizualizaci poskládání všech snímků)\n",
    "\n",
    "## Omezení \n",
    "Jak bylo výše uvedeno, u dlouhých panoramat s jakými pracujeme my, je třeba použít affiní, či podobnostní transformaci, nebot při součinu projektivních transformací dochází ke stále větší a větší chybě, a při více než 5-7 snímcích ani nelze žádné panorama vytvořit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeatability rate\n",
    "\n",
    "[https://hal.inria.fr/inria-00548302/document]\n",
    "\n",
    "![repeat](Images/aaa.png)\n",
    " \n",
    "Opakovatelnost je podle \\cite{repeat} definována jako schopnost detekovat příznaky nezávisle\n",
    "na změnách ve snímacích podmínkách, jako je třeba posun kamery nebo zhoršení kvality obrazu.\n",
    "\n",
    "Míra opakovatelnosti (repeatability rate) jako poměr stejných bodů vyskytujících se na obou snímcích vzhledem k celkovém počtu detekovaných bodů. Z detekovaných bodů vybíráme pouze ty, které jsou součástí scény vyskytujících se na obou snímcích.\n",
    "\n",
    "Je důležité také vzít v úvahu nepřesnost detekce. Korespondující bod se nebude nacházet přesně daném místě určeném maticí, ale v jeho eps-okolí. eps-reapeatabilitu definujeme jako: \n",
    "\n",
    "$$Ri(eps) = {(xi,xj)|dist(Hijxi, xj) < eps}$$\n",
    "\n",
    "Počet detekovaných bodů ve společné části snímku nebude stejný, proto vybereme menší z nich, abychom zajistili existenci bodu z eps-reapeatability. Repeatability rate je tedy definováno následovně: \n",
    "\n",
    "$$ri(eps)=abs(Ri(eps))/min(ni,nj)$$\n",
    "\n",
    "My jako eps-repeatability bude používat inliers získané při odhadu tr. matice RANSAC metodou. Do jmenovatele pak dáme počet korespondujících párů, které leží ve společné části snímku. \n",
    "\n",
    "Epsilon je třeba zvolit tak, aby pravděpodobnost, že se 2 náhodné body nachází ve svém eps-okolí velmi malá. Při rozlišení snímku 1920*1080 a počtu detekovaných bodů pohybujících se okolo 2000 až 200 bude průměrná vzdálenost bodu od sebe mnohem větší, než je eps=10 (většinou se používá 1.5?).\n",
    "\n",
    "Dalším sledovanou hodnotou bude průměrná lokalizační chyba(localization error), což je průmerný vdálenost bodu od jeho predikované pozice, tedy:\n",
    "\n",
    "$$li =  mean(norm(p2(i,:)-predicted(i,:)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vliv rozmazání na registraci\n",
    "\n",
    "Pro sledování vlivu rozmazání na kvalitu registrace použijeme dvojice sousedících snímků, které uměle rozmažeme aplikací motion-blur filtru náhodné orientace v rozsahu <0,190> a pevně zvolené délky v rozsahu <1,50>, kdy je postupně přidávana po inkrementech 5(pixelů?). Z jedné dvojice tak získáme 11 (12 s originálními) párů rozmazaných snímků. U nich pak najdeme příznaky, namatchujeme je a odhadneme homografní matici. Pro E v Eps-repeatabilite dosadíme 10. Takte získáme repeatability rate.\n",
    "\n",
    "\n",
    "Takto projdeme celý daný set snímků. Tady je graf setu 1:\n",
    "(Zavislost rep. na délce rozmazání)\n",
    "<img src='Images/rep.png' />\n",
    "Na vertikální ose je repeatability rate a jeho rozptyl, na horizontální délka filtru. Jak lze vidět z grafu, do cca 30 pixelu je kvalita registrace poměrně spolehlivá, pak ale začne rychle klesat.\n",
    "\n",
    "(závislost rep.rate na loc_error)\n",
    "\n",
    "plot(x,y,'o'/'x')\n",
    "\n",
    "pro vsechny body\n",
    "\n",
    "udelat si mozna fit linearni\n",
    "\n",
    "<img src='Images/rep2loc.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(závislost rep.rate a loc_error na počtu bodu)\n",
    "\n",
    "vyndat nuly z prumeru -> NaN\n",
    "\n",
    "zase je tecky\n",
    "\n",
    "orezat konce (p>1000?)\n",
    "\n",
    "<img align=\"left\" img src=\"Images/p2rep.png\" width=\"400\">\n",
    "<img align=\"left\" img src=\"Images/p2loc.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ratio nepodarenych/delka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tabulka rep. a loc.error pro kazdy set pri pevne danem poctu det bodu(napr 500))\n",
    "\n",
    "pokud byly rozdílné (napr jine telofony)\n",
    "\n",
    "ale jinak ne(mozna u jpeg artefakty)\n",
    "\n",
    "Markdown | Less | Pretty\n",
    "--- | --- | ---\n",
    "*Still* | `renders` | **nicely**\n",
    "1 | 2 | 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vliv artefaktů\n",
    "\n",
    "Sledování vlivu zmenšování datového toku kvalitu registrace.\n",
    "Plánovaný postup:\n",
    "\n",
    "1)Vygeneruju set videí se různým datovým tokem\n",
    "\n",
    "2)Problém s výběrem snímku (od nějakého to už nezvládne vybrat snímek)\n",
    "\n",
    "   řešení: vybírám předem dané snímky (znám čísla z originálu)\n",
    "    \n",
    "3)Dále postup stejný jako u bluru (detekce příznaků, matching, estimateGeometricTransform,etc...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sčítání cenovek\n",
    "\n",
    "V této části si ukážeme využití výše zmíněných metod na problému počítání cenovek. V podstatě jde o schopnost detekce stejného objektu na více snímcích a následném rozdělění tak, aby každý snímek měl \"své\" cenovky. Pro toto je využití vlastností transfromačních matic ideální.\n",
    "\n",
    "Pro samotnou detekci cenovek byla využita typ síte YOLOv5[https://github.com/ultralytics/yolov5] natrénována na cenovky od firmy DataSentics.\n",
    "Ta vyniká především pro svou rychlost i malou velikost, čímž je vhodná pro využití v mobilních zařízeních.\n",
    "Vstupem je snímek a výstupem list souřadnic bounding boxů.\n",
    "\n",
    "Snímky jsou získany z videa postupem výše uvedeném. Mezi těmito snímky je spočítany matice projektivní transformace a je také (odděleně) vygenerováno panorama pomocí affiní transformace. Toto panoram slouží k ručnímu\n",
    "sčítání cenovek.\n",
    "\n",
    "V každém cyklu jsou vzaty 3 po sobě jsoucí snímky, v nichž jsou následně detekovány cenovky. První a třetí bboxy jsou převedeny transformací do souřadného systému druhého snímku. Poté jsou přes Python knihovnu Shapely[] souřadnice bboxů převedeny do polygonů pro snadnější manipulaci. \n",
    "\n",
    "Polygony prvního a druhého snímku jsou dávány přes sebe a pokud je nalezen překryv, polygon je odstraněn z list ve 3. a zůstává pouze ve 1.\n",
    "Následně jsou podobným způsoben kontrolovány polygony 1.&2. a 3.&2. Při nalezení překryvu ale o tom, ze kterého listu bude polygon smazán rozhoduje velikost obsahu cenovky.\n",
    "\n",
    "Po prvním cyklu neprochází již detekované snímky sítí znovu, místo toho jsou brány seznamy z předchozích 2 cyklů.\n",
    "Díky tomu nedochází k duplikaci cenovek na různých snímcích. Mohlo by v nějakých případech dojít k duplikaci, pokud by se objekt objevil na 4 a více snímcích po sobě, nicméně k tomu v podstatě nedochází, vzhledem k tomu že snímky jsou vybrány z videa, tak, aby neměly tak velký překryv. Nehledě na to, že při součinu projektivních transformačních matic se rychle kumuluje chyba po několika cyklech by se bboxy přestaly překrývat.\n",
    "\n",
    "\n",
    "## Omezení\n",
    "Sít má problém s cenovkami velmi blízko u sebe a najde neexistující cenovky. Také má problém správně detekovat cenovky vyloženě přes sebe a neobvyklých formátů(elektronické, moc dlouhé)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Výsledky\n",
    "ručně/program\n",
    "\n",
    "set30-188/190\n",
    "\n",
    "set34-102/96\n",
    "\n",
    "mít 5 setů - vizualizace pomocí panoramatu s cenovkami přes\n",
    "<img src='Images/set30.png' />\n",
    "<img src='Images/set34.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
